{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfeK7uNVntwz"
      },
      "source": [
        "# Converting a model with TF Lite converter\n",
        "\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/toelt-llc/HSLU-WSCS_2025/blob/master/05%20-%20Converting_a_model_with_TF_Lite_converter.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>\n",
        "\n",
        "(C) Umberto Michelucci\n",
        "\n",
        "umberto.michelucci@toelt.ai\n",
        "\n",
        "www.toelt.ai\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ybdPIc1Z5vn"
      },
      "source": [
        "First we simply import the packages we need. Note that we want to use `TensorFlow 2.0` and therefore we use the magic command `%tensorflow_version 2.x`. Note that this works only in Google colab, and no if you are using it on a local installation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OhsHdS2Kn46b"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chzXx0SwaG-v"
      },
      "source": [
        "Is always a good idea to check the version of `TensorFlow` that you are really using, to make sure you get what you need."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0QtvnmHZoAfJ",
        "outputId": "126b6bf5-b81a-439d-8b33-2a6ab6a0f01d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.12.0\n"
          ]
        }
      ],
      "source": [
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W123vWD1Zk5A"
      },
      "source": [
        "We first download, from the package `tf.keras.applications` the `MobileNetV2` pretrained network that we will use in this example. Note how we give the parameter `weights=\"imagenet\"` that means we want to get the entire network with all the weights after the training with the `imagenet` dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5IyobHeToCAK",
        "outputId": "9bee4e11-8b09-4100-ddeb-652b2c01cc17"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224.h5\n",
            "14536120/14536120 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "model = tf.keras.applications.MobileNetV2(\n",
        "    weights=\"imagenet\", input_shape=(224, 224, 3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uu1qAr50oLQ-"
      },
      "source": [
        "Now let's use the converted model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OzQ0z-RKaO0D"
      },
      "source": [
        "The most basic usage of `TensorFlow Lite` is simply to instantiate a `converter`, give the `optimizations` options, and then convert the model with `convert.convert()`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zA9sgXka0PG"
      },
      "source": [
        "## Model conversion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oxo3NsPyiRFF",
        "outputId": "fdf52122-e17d-4cc5-a938-ccac548159b5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 52). These functions will not be directly callable after loading.\n"
          ]
        }
      ],
      "source": [
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "#converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "converter.experimental_new_converter = True\n",
        "tflite_model = converter.convert()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvxKa0kNapay"
      },
      "source": [
        "## Inference with the converted model\n",
        "\n",
        "Now to do inference with the converted model, we first need to instantiate an `interpreter`, then allocate the tensors necessary (for input and output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MadMMkWDamc3"
      },
      "outputs": [],
      "source": [
        "interpreter = tf.lite.Interpreter(model_content=tflite_model)\n",
        "interpreter.allocate_tensors()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4GgpHlTa8nN"
      },
      "source": [
        "Now we can simply get the information on the inputs (for example the shape) and the outputs of the model with the following code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZ-j5UHDokp4"
      },
      "outputs": [],
      "source": [
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F38gD9oGomMh",
        "outputId": "364b1bdc-218f-4d82-de5d-622afc073f53"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'name': 'serving_default_input_1:0', 'index': 0, 'shape': array([  1, 224, 224,   3], dtype=int32), 'shape_signature': array([ -1, 224, 224,   3], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n"
          ]
        }
      ],
      "source": [
        "print(input_details)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4dkR_rIbD0j"
      },
      "source": [
        "Looking at the output you can see that the output has the shape `[1,1000]`, and that means that the model has been trained on `1000` classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SXQiZf6BonaH",
        "outputId": "5a16b98e-2b15-42ba-af20-8f5632063ff9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'name': 'StatefulPartitionedCall:0', 'index': 177, 'shape': array([   1, 1000], dtype=int32), 'shape_signature': array([  -1, 1000], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n"
          ]
        }
      ],
      "source": [
        "print(output_details)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bm8iecDjbZIr"
      },
      "source": [
        "The last step for doing inference is to get the input shape with\n",
        "\n",
        "`input_details[0]['shape']`\n",
        "\n",
        "and then just for testing purposes we get an array with random values with the right shape and then `invoke()` the `interpreter`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qfqLGPaFopbg"
      },
      "outputs": [],
      "source": [
        "input_shape = input_details[0]['shape']\n",
        "input_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)\n",
        "interpreter.set_tensor(input_details[0]['index'], input_data)\n",
        "\n",
        "interpreter.invoke()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZn_pCEXfXIY"
      },
      "source": [
        "This will get us the resulting tensor. So an array of 1000 values that will contain the probabilities for the input to be a specific class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rS7_dHDqotBb",
        "outputId": "590abb4a-083e-4903-e482-5da95b2d3eb2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1, 1000)\n"
          ]
        }
      ],
      "source": [
        "tflite_results = interpreter.get_tensor(output_details[0]['index'])\n",
        "print(tflite_results.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2JyZ7I3covqM",
        "outputId": "958f2dee-cec6-4330-996e-6d6386bfc08c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1, 1000)\n"
          ]
        }
      ],
      "source": [
        "tf_results = model(tf.constant(input_data))\n",
        "print(tf_results.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LZaREsGfetT"
      },
      "source": [
        "This line will check that the result from the original model and the converted one are equal up to the 5th digit. Note that if you do quantization this will not be true anymore!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tiUYAc4ooxD7"
      },
      "outputs": [],
      "source": [
        "for tf_result, tflite_result in zip(tf_results, tflite_results):\n",
        "  np.testing.assert_almost_equal(tf_result, tflite_result, decimal=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0VcznmjjDGK"
      },
      "source": [
        "## Estimation of model size on disk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z2-u0bIeoz52",
        "outputId": "37fbe530-3b6c-4265-9410-68f8240a4982"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "13986741\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "\n",
        "size_estimate = len(pickle.dumps(tflite_model))\n",
        "print(size_estimate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "139LDhvLhWRG"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
