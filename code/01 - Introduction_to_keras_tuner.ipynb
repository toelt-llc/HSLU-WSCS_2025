{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tce3stUlHN0L"
   },
   "source": [
    "# Introduction to Keras Tuner\n",
    "\n",
    "Adapted and expanded from the original code by Umberto Michelucci.\n",
    "\n",
    "Original code Copyright 2020 The TensorFlow Authors\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/toelt-llc/HSLU-WSCS_2025/blob/master/01%20-%20Introduction_to_keras_tuner.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "form",
    "execution": {
     "iopub.execute_input": "2023-12-07T03:19:58.040143Z",
     "iopub.status.busy": "2023-12-07T03:19:58.039470Z",
     "iopub.status.idle": "2023-12-07T03:19:58.043698Z",
     "shell.execute_reply": "2023-12-07T03:19:58.043061Z"
    },
    "id": "tuOe1ymfHZPu"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qFdPvlXBOdUN"
   },
   "source": [
    "# Introduction to the Keras Tuner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xHxb-dlhMIzW"
   },
   "source": [
    "## Overview\n",
    "\n",
    "`keras.tuner` is a module within the Keras ecosystem, a popular open-source library for deep learning in Python. The primary purpose of `keras.tuner` is to perform hyperparameter tuning for Keras models, which is an essential step in optimizing machine learning models for better performance.\n",
    "\n",
    "Here are the key aspects of `keras.tuner`:\n",
    "\n",
    "1. **Functionality**: `keras.tuner` provides a simple and efficient way to find the best hyperparameter values for your Keras models. Hyperparameters include choices about the number of layers, their types, the number of neurons in each layer, learning rate, activation functions, and more.\n",
    "\n",
    "2. **Tuners Available**: It offers several tuning algorithms, including Random Search, Hyperband, and Bayesian Optimization. Each of these tuners has its own strategy for exploring the hyperparameter space.\n",
    "   - **Random Search**: Tests a random selection of hyperparameter values within the predefined search space.\n",
    "   - **Hyperband**: An optimized version of random search which uses adaptive resource allocation and early-stopping to quickly converge on a high-performing model.\n",
    "   - **Bayesian Optimization**: Models the function mapping from hyperparameters to a target score and uses this model to select promising hyperparameters to evaluate in the real world.\n",
    "\n",
    "3. **Ease of Integration**: Designed to integrate seamlessly with Keras models, `keras.tuner` makes it relatively straightforward to add hyperparameter tuning to your existing model-building workflow.\n",
    "\n",
    "4. **Customization**: Users can define their own search space for hyperparameters, allowing for extensive customization and experimentation. This includes setting parameters like the number of layers, the number of units in each layer, learning rates, and other model hyperparameters.\n",
    "\n",
    "5. **Search Process**: During the search process, `keras.tuner` systematically tests different combinations of hyperparameters to find the combination that yields the best performance on a validation dataset.\n",
    "\n",
    "6. **Results Analysis**: After the tuning process, it provides detailed results about each trial (set of hyperparameters) including its performance, which can be analyzed to understand how different hyperparameters affect model performance.\n",
    "\n",
    "7. **Practical Applications**: `keras.tuner` is widely used in deep learning projects where finding the right set of hyperparameters is crucial for model performance, such as in image recognition, natural language processing, and predictive modeling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MUXex9ctTuDB"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install keras_tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-07T03:19:58.047090Z",
     "iopub.status.busy": "2023-12-07T03:19:58.046855Z",
     "iopub.status.idle": "2023-12-07T03:20:00.418703Z",
     "shell.execute_reply": "2023-12-07T03:20:00.418001Z"
    },
    "id": "IqR2PQG4ZaZ0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-27 07:57:42.038024: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from tensorflow import keras\n",
    "import keras_tuner as kt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g83Lwsy-Aq2_"
   },
   "source": [
    "Install and import the Keras Tuner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-07T03:20:00.423105Z",
     "iopub.status.busy": "2023-12-07T03:20:00.422681Z",
     "iopub.status.idle": "2023-12-07T03:20:02.778604Z",
     "shell.execute_reply": "2023-12-07T03:20:02.777563Z"
    },
    "id": "hpMLpbt9jcO6"
   },
   "outputs": [],
   "source": [
    "# Run this only if necessary\n",
    "# !pip install -q -U keras-tuner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ReV_UXOgCZvx"
   },
   "source": [
    "## Download and prepare the dataset\n",
    "\n",
    "In this tutorial, you will use the Keras Tuner to find the best hyperparameters for a machine learning model that classifies images of clothing from the [Fashion MNIST dataset](https://github.com/zalandoresearch/fashion-mnist)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Description\n",
    "\n",
    "The Zalando MNIST dataset, also known as the Fashion-MNIST dataset, is a dataset comprising of 70,000 grayscale images of 10 different fashion products from Zalando, a large European e-commerce company. It was created as a more challenging replacement for the traditional MNIST dataset of handwritten digits. Here are some key details about the Fashion-MNIST dataset:\n",
    "\n",
    "1. **Content**: The dataset contains 70,000 grayscale images, each 28x28 pixels, divided into 10 fashion categories such as T-shirts/tops, trousers, pullovers, dresses, coats, sandals, shirts, sneakers, bags, and ankle boots.\n",
    "\n",
    "2. **Training and Testing Split**: Similar to the original MNIST, it includes 60,000 training images and 10,000 test images. This standard split facilitates consistent evaluation of machine learning models.\n",
    "\n",
    "3. **Purpose**: Fashion-MNIST was designed to serve as a direct drop-in replacement for the original MNIST dataset for benchmarking machine learning algorithms. It introduces more complexity compared to the original MNIST, making it a better representative of modern computer vision tasks.\n",
    "\n",
    "4. **Use Cases**: It's widely used for machine learning and computer vision tasks like classification, image recognition, and machine learning model performance evaluation.\n",
    "\n",
    "5. **Accessibility and Usability**: Like MNIST, Fashion-MNIST is easily accessible and can be used with common machine learning libraries. It's suitable for both beginners and advanced researchers, providing a more challenging dataset than MNIST while maintaining a similar size and structure.\n",
    "\n",
    "6. **Benchmarking**: Since its introduction, Fashion-MNIST has been adopted by the machine learning community as a benchmark dataset, often used in academic papers and machine learning competitions to evaluate the performance of various algorithms.\n",
    "\n",
    "7. **Educational Value**: For educational purposes, Fashion-MNIST offers a more complex challenge than MNIST while being more comprehensible and visually interpretable than more complex datasets like ImageNet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HljH_ENLEdHa"
   },
   "source": [
    "Load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-07T03:20:03.136168Z",
     "iopub.status.busy": "2023-12-07T03:20:03.135094Z",
     "iopub.status.idle": "2023-12-07T03:20:03.542925Z",
     "shell.execute_reply": "2023-12-07T03:20:03.542192Z"
    },
    "id": "OHlHs9Wj_PUM"
   },
   "outputs": [],
   "source": [
    "(img_train, label_train), (img_test, label_test) = keras.datasets.fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-07T03:20:03.547320Z",
     "iopub.status.busy": "2023-12-07T03:20:03.546629Z",
     "iopub.status.idle": "2023-12-07T03:20:03.646142Z",
     "shell.execute_reply": "2023-12-07T03:20:03.645354Z"
    },
    "id": "bLVhXs3xrUD0"
   },
   "outputs": [],
   "source": [
    "# Normalize pixel values between 0 and 1\n",
    "img_train = img_train.astype('float32') / 255.0\n",
    "img_test = img_test.astype('float32') / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K5YEL2H2Ax3e"
   },
   "source": [
    "## Define the model\n",
    "\n",
    "When you build a model for hypertuning, you also define the hyperparameter search space in addition to the model architecture. The model you set up for hypertuning is called a *hypermodel*.\n",
    "\n",
    "You can define a hypermodel through two approaches:\n",
    "\n",
    "* By using a model builder function\n",
    "* By subclassing the `HyperModel` class of the Keras Tuner API\n",
    "\n",
    "You can also use two pre-defined [HyperModel](https://keras.io/api/keras_tuner/hypermodels/) classes - [HyperXception](https://keras.io/api/keras_tuner/hypermodels/hyper_xception/) and [HyperResNet](https://keras.io/api/keras_tuner/hypermodels/hyper_resnet/) for computer vision applications.\n",
    "\n",
    "In this tutorial, you use a model builder function to define the image classification model. The model builder function returns a compiled model and uses hyperparameters you define inline to hypertune the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-07T03:20:03.650529Z",
     "iopub.status.busy": "2023-12-07T03:20:03.650216Z",
     "iopub.status.idle": "2023-12-07T03:20:03.655939Z",
     "shell.execute_reply": "2023-12-07T03:20:03.655314Z"
    },
    "id": "ZQKodC-jtsva"
   },
   "outputs": [],
   "source": [
    "def model_builder(hp):\n",
    "  model = keras.Sequential()\n",
    "  model.add(keras.layers.Flatten(input_shape=(28, 28)))\n",
    "\n",
    "  # Tune the number of units in the first Dense layer\n",
    "  # Choose an optimal value between 32-512\n",
    "  hp_units = hp.Int('units', min_value=32, max_value=512, step=32)\n",
    "  model.add(keras.layers.Dense(units=hp_units, activation='relu'))\n",
    "  model.add(keras.layers.Dense(10))\n",
    "\n",
    "  # Tune the learning rate for the optimizer\n",
    "  # Choose an optimal value from 0.01, 0.001, or 0.0001\n",
    "  hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "\n",
    "  model.compile(optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
    "                loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0J1VYw4q3x0b"
   },
   "source": [
    "## Hyperparameter with Hyperband\n",
    "\n",
    "Instantiate the tuner to perform the hypertuning. The Keras Tuner has four tuners available - `RandomSearch`, `Hyperband`, `BayesianOptimization`, and `Sklearn`. In this tutorial, we use the [Hyperband](https://arxiv.org/pdf/1603.06560.pdf) tuner.\n",
    "\n",
    "To instantiate the Hyperband tuner, you must specify the hypermodel, the `objective` to optimize and the maximum number of epochs to train (`max_epochs`).\n",
    "\n",
    "Hyperband in Keras, specifically integrated through `keras.tuner`, is an implementation of the Hyperband hyperparameter tuning algorithm designed for optimizing hyperparameters in Keras models. It's an advanced, efficient method particularly suited for large hyperparameter spaces and complex models. Here's a detailed description of Hyperband in the context of Keras:\n",
    "\n",
    "1. **Algorithm Overview**: \n",
    "   - Hyperband is based on the concept of adaptive resource allocation and early-stopping. It is an extension of Random Search but incorporates a systematic way to decide how many resources (like epochs) to allocate to each trial (set of hyperparameters) and when to stop underperforming trials.\n",
    "\n",
    "2. **Efficient Exploration**: \n",
    "   - Unlike traditional methods that evaluate each hyperparameter combination for a fixed amount of resources, Hyperband dynamically allocates resources. It starts by evaluating many configurations with a small amount of resources and progressively gives more resources to promising configurations in subsequent rounds.\n",
    "\n",
    "3. **Integration with Keras**: \n",
    "   - In Keras, `Hyperband` is provided through the `keras.tuner` module. It is designed to work seamlessly with Keras models, allowing for easy specification of the model architecture and the hyperparameters to tune.\n",
    "\n",
    "4. **Key Parameters**: \n",
    "   - `max_epochs`: The maximum number of epochs to train a single model. It's the upper limit of resources that Hyperband can allocate to any trial.\n",
    "   - `objective`: The metric to be optimized, which could be a standard metric like accuracy or a custom-defined function.\n",
    "   - `factor`: The reduction factor that decides how much the number of configurations is reduced in each round.\n",
    "   - `hyperband_iterations`: The number of times to run the hyperband algorithm (each with different random seeds).\n",
    "\n",
    "5. **Process**: \n",
    "   - Hyperband runs in a series of \"brackets\". Each bracket comprises multiple rounds of training and evaluation, where each subsequent round trains fewer models for more epochs.\n",
    "   - Initially, a large number of models are trained for a small number of epochs. Only the top-performing models (as per the specified `objective`) proceed to the next round, where they are trained for longer. This process repeats, reducing the number of models and increasing the epochs each time, until the best-performing models are identified.\n",
    "\n",
    "6. **Advantages**: \n",
    "   - Hyperband is particularly effective when dealing with large datasets and complex models because it quickly discards poor-performing configurations.\n",
    "   - It can significantly reduce the computational cost and time required for hyperparameter tuning compared to traditional methods.\n",
    "\n",
    "7. **Usage in Keras**: \n",
    "   - To use Hyperband in Keras, you define a model-building function, specify the hyperparameter space, and then pass these to the `Hyperband` tuner. The tuner then manages the training and evaluation process, providing you with the best hyperparameters found.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-07T03:20:03.659627Z",
     "iopub.status.busy": "2023-12-07T03:20:03.658917Z",
     "iopub.status.idle": "2023-12-07T03:20:05.979646Z",
     "shell.execute_reply": "2023-12-07T03:20:05.978757Z"
    },
    "id": "oichQFly6Y46"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from my_dir/intro_to_kt/tuner0.json\n"
     ]
    }
   ],
   "source": [
    "tuner = kt.Hyperband(model_builder,\n",
    "                     objective='val_accuracy',\n",
    "                     max_epochs=10,\n",
    "                     factor=3,\n",
    "                     directory='my_dir',\n",
    "                     project_name='intro_to_kt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VaIhhdKf9VtI"
   },
   "source": [
    "The Hyperband tuning algorithm uses adaptive resource allocation and early-stopping to quickly converge on a high-performing model. This is done using a sports championship style bracket. The algorithm trains a large number of models for a few epochs and carries forward only the top-performing half of models to the next round. Hyperband determines the number of models to train in a bracket by computing 1 + log<sub>`factor`</sub>(`max_epochs`) and rounding it up to the nearest integer.\n",
    "\n",
    "Early stopping is a regularization technique (not quite but let's go with that) used in training neural networks to prevent overfitting. It involves monitoring the performance of the model on a validation dataset and stopping the training process when the performance starts to degrade or stops improving. Here's a detailed description of the early stopping approach:\n",
    "\n",
    "1. **Objective**: The primary goal of early stopping is to halt the training at the point when the model is generalized enough to perform well on unseen data, but before it starts to overfit the training data.\n",
    "\n",
    "2. **How It Works**:\n",
    "   - During training, the model's performance is continually evaluated on a separate validation dataset that is not used for the actual training.\n",
    "   - After each epoch (or a set number of epochs), the algorithm checks how the model's performance on the validation set has changed.\n",
    "   - If the model's performance on the validation set improves or remains the same, training continues.\n",
    "   - If the model's performance on the validation set starts to worsen (e.g., the validation loss starts to increase), it's a sign that the model may be beginning to overfit the training data.\n",
    "\n",
    "3. **Stopping Criteria**:\n",
    "   - A common criterion is to stop training when the validation loss has not decreased for a specified number of epochs, often referred to as the \"patience\" parameter.\n",
    "   - Alternatively, training can be stopped based on other metrics, such as accuracy or F1 score, depending on the specific task.\n",
    "\n",
    "4. **Restoring the Best Model**:\n",
    "   - When early stopping is triggered, it's common practice to restore the weights of the model to the state when it performed the best on the validation set. This ensures that the model retains the generalization capability it had before it started overfitting.\n",
    "\n",
    "5. **Benefits**:\n",
    "   - **Prevents Overfitting**: By stopping the training before the model overfits the data, early stopping helps in maintaining the model's ability to generalize to new data.\n",
    "   - **Saves Time and Resources**: It reduces the number of unnecessary training epochs, saving computational resources and time.\n",
    "   - **Automatic and Simple**: It's an automated approach that doesn't require manual intervention and is easy to implement in most deep learning frameworks.\n",
    "\n",
    "6. **Implementation in Deep Learning Frameworks**:\n",
    "   - Early stopping is supported in many deep learning frameworks as a built-in function. In frameworks like TensorFlow/Keras, it is implemented as a callback function that can be easily added to the training process.\n",
    "\n",
    "7. **Tuning Early Stopping**:\n",
    "   - The patience parameter and the specific metric to monitor are crucial aspects of early stopping and may require tuning based on the dataset and problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cwhBdXx0Ekj8"
   },
   "source": [
    "Create a callback to stop training early after reaching a certain value for the validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-07T03:20:05.984275Z",
     "iopub.status.busy": "2023-12-07T03:20:05.983977Z",
     "iopub.status.idle": "2023-12-07T03:20:05.987832Z",
     "shell.execute_reply": "2023-12-07T03:20:05.987132Z"
    },
    "id": "WT9IkS9NEjLc"
   },
   "outputs": [],
   "source": [
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UKghEo15Tduy"
   },
   "source": [
    "Run the hyperparameter search. The arguments for the search method are the same as those used for `tf.keras.model.fit` in addition to the callback above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-07T03:20:05.991445Z",
     "iopub.status.busy": "2023-12-07T03:20:05.990814Z",
     "iopub.status.idle": "2023-12-07T03:28:48.512022Z",
     "shell.execute_reply": "2023-12-07T03:28:48.511228Z"
    },
    "id": "dSBQcTHF9cKt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The hyperparameter search is complete. The optimal number of units in the first densely-connected\n",
      "layer is 448 and the optimal learning rate for the optimizer\n",
      "is 0.001.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "#\n",
    "# CAREFUL: IT TAKES SOME TIME!\n",
    "#\n",
    "#\n",
    "tuner.search(img_train, label_train, epochs=5, validation_split=0.2, callbacks=[stop_early])\n",
    "\n",
    "# Get the optimal hyperparameters\n",
    "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(f\"\"\"\n",
    "The hyperparameter search is complete. The optimal number of units in the first densely-connected\n",
    "layer is {best_hps.get('units')} and the optimal learning rate for the optimizer\n",
    "is {best_hps.get('learning_rate')}.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lak_ylf88xBv"
   },
   "source": [
    "## Train the model\n",
    "\n",
    "Find the optimal number of epochs to train the model with the hyperparameters obtained from the search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-07T03:28:48.515436Z",
     "iopub.status.busy": "2023-12-07T03:28:48.515165Z",
     "iopub.status.idle": "2023-12-07T03:31:57.753169Z",
     "shell.execute_reply": "2023-12-07T03:31:57.752415Z"
    },
    "id": "McO82AXOuxXh"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arnaud/miniconda3/envs/wscs/lib/python3.12/site-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.7838 - loss: 0.6131 - val_accuracy: 0.8549 - val_loss: 0.3952\n",
      "Epoch 2/5\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.8649 - loss: 0.3737 - val_accuracy: 0.8738 - val_loss: 0.3531\n",
      "Epoch 3/5\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.8794 - loss: 0.3291 - val_accuracy: 0.8804 - val_loss: 0.3340\n",
      "Epoch 4/5\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.8862 - loss: 0.3034 - val_accuracy: 0.8727 - val_loss: 0.3557\n",
      "Epoch 5/5\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.8949 - loss: 0.2837 - val_accuracy: 0.8801 - val_loss: 0.3304\n",
      "Best epoch: 3\n"
     ]
    }
   ],
   "source": [
    "# Build the model with the optimal hyperparameters and train it on the data for 50 epochs\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "history = model.fit(img_train, label_train, epochs=5, validation_split=0.2)\n",
    "\n",
    "val_acc_per_epoch = history.history['val_accuracy']\n",
    "best_epoch = val_acc_per_epoch.index(max(val_acc_per_epoch)) + 1\n",
    "print('Best epoch: %d' % (best_epoch,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uOTSirSTI3Gp"
   },
   "source": [
    "Re-instantiate the hypermodel and train it with the optimal number of epochs from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-07T03:31:57.757140Z",
     "iopub.status.busy": "2023-12-07T03:31:57.756644Z",
     "iopub.status.idle": "2023-12-07T03:34:44.271329Z",
     "shell.execute_reply": "2023-12-07T03:34:44.270534Z"
    },
    "id": "NoiPUEHmMhCe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.7865 - loss: 0.6167 - val_accuracy: 0.8530 - val_loss: 0.4070\n",
      "Epoch 2/3\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.8604 - loss: 0.3803 - val_accuracy: 0.8590 - val_loss: 0.3851\n",
      "Epoch 3/3\n",
      "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.8753 - loss: 0.3353 - val_accuracy: 0.8724 - val_loss: 0.3523\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x14545c770>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypermodel = tuner.hypermodel.build(best_hps)\n",
    "\n",
    "# Retrain the model\n",
    "hypermodel.fit(img_train, label_train, epochs=best_epoch, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MqU5ZVAaag2v"
   },
   "source": [
    "To finish this tutorial, evaluate the hypermodel on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-07T03:34:44.275340Z",
     "iopub.status.busy": "2023-12-07T03:34:44.274641Z",
     "iopub.status.idle": "2023-12-07T03:34:45.100071Z",
     "shell.execute_reply": "2023-12-07T03:34:45.099323Z"
    },
    "id": "9E0BTp9Ealjb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8661 - loss: 0.3666\n",
      "[test loss, test accuracy]: [0.37075960636138916, 0.866100013256073]\n"
     ]
    }
   ],
   "source": [
    "eval_result = hypermodel.evaluate(img_test, label_test)\n",
    "print(\"[test loss, test accuracy]:\", eval_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EQRpPHZsz-eC"
   },
   "source": [
    "The `my_dir/intro_to_kt` directory contains detailed logs and checkpoints for every trial (model configuration) run during the hyperparameter search. If you re-run the hyperparameter search, the Keras Tuner uses the existing state from these logs to resume the search. To disable this behavior, pass an additional `overwrite=True` argument while instantiating the tuner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sKwLOzKpFGAj"
   },
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial, you learned how to use the Keras Tuner to tune hyperparameters for a model. To learn more about the Keras Tuner, check out these additional resources:\n",
    "\n",
    "* [Keras Tuner on the TensorFlow blog](https://blog.tensorflow.org/2020/01/hyperparameter-tuning-with-keras-tuner.html)\n",
    "* [Keras Tuner website](https://keras-team.github.io/keras-tuner/)\n",
    "\n",
    "Also check out the [HParams Dashboard](https://www.tensorflow.org/tensorboard/hyperparameter_tuning_with_hparams) in TensorBoard to interactively tune your model hyperparameters (in case you are using TensorBoard)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "Tce3stUlHN0L"
   ],
   "name": "keras_tuner.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "wscs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
